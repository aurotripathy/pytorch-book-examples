{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitpyt12condaa6d578b60c2b45e7882cc9b9680228e7",
   "display_name": "Python 3.8.2 64-bit ('pyt1.2': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 1 — Classifying Linearly Separable Data with Neural Nets\n",
    "## Introduction\n",
    "Your journey to understand Neural Networks starts with a humble example of building a binary classifier for linearly separable data. 'Linearly seperable data'  means that a line can shatter (or classify) the data into two classes. For two-dimensional data, the decision boundary is a line, and for three or more dimensions in the data, the decision boundary would be a plane and a milti-dimensional daat the decision boundary would be a plane.\n",
    "\n",
    "## The Dataset\n",
    "We’ll use the example below to illustrate the workings at the Neural Network. Imagine you are running a computing infrastructure where you have just two tiers of pricing, VIP and Regular. You want to classify each user into one of the two tiers (or classes) based on the two resources they consume. Let's call these resources x1 and x2 (for instance, the resources could be the number of CPU cores and the network bandwidth). Also, you are just starting out providing such a service and you have a only handful of users (i.e., a small number of data samples). Your task is to draw the decision boundary between the two classes of users. The decision boundary is a straight line and you are granted the assumption that the data is linearly separable.\n",
    "\n",
    "![startup pic](plots/first-diag.png)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A common practice is to center the data around the origin (0, 0) by taking the mean and subtracting each data point from the mean.\n",
    "\n",
    "![](plots/centered-data.png)\n",
    "\n",
    "The decision boundary could be anywhere in the shaded area.\n",
    "All the data points in the graph above is shown below (in the form of a Python list). Each element consists of the two resources (x1, x2) and the class the user belongs to (0 or 1).\n",
    " \n",
    "## The Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\"\"\"\n",
    "Binary Classifier with Linear layer and Back Propagation of Errors\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optimizer\n",
    "from utils import display_loss, display_points, plot_points_line_slope_intercept\n",
    "\n",
    "print(f'Torch version: {torch.__version__}')\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Torch version: 1.7.1\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Since the data is linearly separable, you must estimate the parameters, w1, w2 and b that represent the line:\n",
    "\n",
    "w1* x1 + w2 * x2 + b \n",
    "\n",
    "or \n",
    "\n",
    "WX + b.  \n",
    "\n",
    "X is the input vector of size two consisting of the scalars x1, and x2. \n",
    "\n",
    "W is the weights vector consisting of coefficients w1, w2. \n",
    "\n",
    "The bias term is denoted as b. \n",
    "\n",
    "The Neural Network, shown below has one linear layer (that's it!). It does not have any non-linearity (such as Sigmoid or ReLU) added to it. The reason it, the task of building a decision boundary on the (toy) data we chose is simple and does not require more complexity than necessary. Note, even if the network does not have any non-linear activation, we call this a neural network because we use the principles of back propagation to estimate the model parameters, w1, w2 and b.\n",
    "\n",
    "We’re going to learn the model parameters, w1, w2, and b by the technique of backpropagation of errors. That means we need a loss function at the head of the network.\n",
    "\n",
    "The network is represented in the Fig 1.\n",
    "\n",
    "![network](plots/network.png)\n",
    "\n",
    "Fig 1. The inputs x1 and x2 along with the corresponding class label is fed into a Linear layer. The arrows indicate the forward path of the single layer Neural Network.\n",
    "\n",
    "The Python class describing the above network in PyTorch framework is as follows. \n",
    "\n",
    "Note, we define only the forward path thru the network and allow the PyTorch \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\" One Linear layer \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fully_connected = Linear(2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fully_connected(x)  # WX + b"
   ]
  },
  {
   "source": [
    "The reason we do not state anything about the baskward pass is that the torch.autograd package provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\n",
    "\n",
    "The class, Linear applies a linear transformation with WT to the incoming data, X and adds a bias term.\n",
    "\n",
    "XWT + b\n",
    "\n",
    "Here, the size of each input sample is 2 and the size of the output is 1.\n",
    "\n",
    "Setting up the Neural Network\n",
    "\n",
    "We instantiate the model, define the loss function (mean-squared error, MSE for short), and define the optimizer as Stochastic Gradient Descent (SGD) with a learning rate (lr) of 0.01. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optimizer.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "train_set = [((-2, -1), 0), ((-2, 1), 1), ((-1, -1.5), 0),\n",
    "             ((1, 1), 1), ((1.5, -0.5), 1), ((2, -2), 0)]\n",
    "display_points([sample[0] for sample in train_set],\n",
    "               [sample[1] for sample in train_set], \"Data\")\n"
   ]
  },
  {
   "source": [
    "We also define the training set consisting of only the six point in our dataset.\n",
    "\n",
    "## Training the Neural Network\n",
    "We train the model for 50 epochs with an often-used “training” recipe. The recipe constitutes the following steps:\n",
    "\n",
    "⎯\tZeroing out the gradients\n",
    "\n",
    "⎯\tMaking a forward pass thru the model with a batch of input data\n",
    "\n",
    "⎯\tApplying the loss at the head of the network (the mean square error between the predicted value and the target value) \n",
    "\n",
    "⎯\tComputing the gradients via back propagation (in this case, backpropagating thru just the one hidden layer)\n",
    "\n",
    "⎯\tUpdating the model parameters.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_over_epochs = []\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for train_data in train_set:\n",
    "        X = torch.tensor([train_data[0]], dtype=torch.float, requires_grad=True)\n",
    "        y = torch.tensor([train_data[1]], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "        optimizer.zero_grad() # Zero out for each batch\n",
    "        y_pred = model(X)     # Forward Propagation\n",
    "        loss = criterion(torch.squeeze(y_pred, 1), y)  # Compute loss\n",
    "        loss.backward()       # Compute gradient\n",
    "        optimizer.step()      # Update model parameters\n",
    "        epoch_loss += loss\n",
    "\n",
    "    loss_over_epochs.append(epoch_loss)\n",
    "    print('Epoch {}, Epoch loss:{}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "source": [
    "As shown, we iterate over all the data and all the epochs with a nested for loop.\n",
    "\n",
    "On our case, we use a batch-size of one for simplification (but doesn’t need to be so, if we had a lot of data). Also, in our case, the computation is performed on the CPU (the default behavior). Later, we’ll show the extra steps needed to move the computations onto the GPU.  \n",
    "\n",
    "Putting the training thru its paces generate a loss plot that looks quite typical in its convergence characteristics perhaps indicating that the choice of the hyperparameters (learning rate and momentum) is adequate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss(loss_over_epochs, \"Loss Plot\")\n",
    "print('Model params:', list(model.parameters()))  # https://graphsketch.com/"
   ]
  },
  {
   "source": [
    "The model parameters give us the values for w1, w2, and b. We use this later to display the decision boundary.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w1, w2) = model.fully_connected.weight.data.numpy()[0]\n",
    "b = model.fully_connected.bias.data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_line_slope_intercept([sample[0] for sample in train_set],\n",
    "                                 [sample[1] for sample in train_set],\n",
    "                                 -w1/w2, -b, 'Decision Boundary')\n",
    "\n",
    "    \n",
    "# Test on training data\n",
    "for train_data in train_set:\n",
    "    prob = model(torch.tensor([train_data[0]], dtype=torch.float, requires_grad=False))\n",
    "    label = 0 if prob < 0.5 else 1\n",
    "    verdict = 'correct' if label == train_data[1] else 'wrong'\n",
    "    print('Data in:{}, Actual Class:{} Out Score:{}, Predicted Class{}: {}'.format(train_data,\n",
    "                                                                                   train_data[1],\n",
    "                                                                                   prob,\n",
    "                                                                                   label,\n",
    "                                                                                   verdict))"
   ]
  }
 ]
}