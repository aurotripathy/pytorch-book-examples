{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this chapter, we’ll illustrate the basics of multi-label classification with an example. But first, we want to distinguish between two kinds of classification, multi-class classification and multi-label classification.\n",
    "## Multi-class Classification \n",
    "The model will categorize/classify the input image into one of several classes (thus, multi-class), i.e.,the input image belongs to one and only one class out of several. For example, let's say the image has a dominant object  of an airplane in the foreground  and perhaps  smaller objects (say, trucks) in the background. This image will be classified into the single category of airplane. \n",
    "\n",
    "\n",
    "\n",
    "![desert](assets/desert+mountains-label-desert.png)\n",
    "\n",
    "For computing the loss for multi-class classification, it's convenient to use the torch.nn.CrossEntropyLoss class which combines nn.LogSoftmax() and nn.NLLLoss() into one single class.\n",
    "\n",
    "## Multi-label Classification \n",
    "“Multi-label” classification means that each image can belong to any\n",
    "number of the specified classes, including zero (background). So multi-label\n",
    "classification can be understood as a series of binary classifications per class.\n",
    "Is the image in class A – yes or no? \n",
    "Is the same image in class B – yes or no?\n",
    "And so on.\n",
    "This is how we end up with multiple labels/classes for a single image.\n",
    "\n",
    "![desert+mountains](assets/desert+mountains-label-desert+mountains.png)\n",
    "\n",
    "For computing the loss for multi-label classification, it's convenient to use the torch.nn.BCEWithLogitsLoss class which combines a Sigmoid layer and the BCELoss (Binary Cross Entropy Loss) in one single class. By combining the operations into one layer, we take advantage of numerical stability inherent in these combined operations (and this is well documented).\n",
    "\n",
    "We note that, in both types of classifications, the rest of the network layers are unchanged (only the loss function at the head of the network changes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download\n",
    "\n",
    "The details of the dataset are available at http://www.lamda.nju.edu.cn/data_MIMLimage.ashx\n",
    "As you would expect, the dataset has two parts; the images of the scenes and the corresponding labels:\n",
    "(1)\tThe \"original\" part contains the 2000 images. An image can belong to one or more classes.  The classes happen to be ['desert', 'mountains', 'sea', 'sunset', 'trees']\n",
    "(2) The \"processed\" part contains the labels.\n",
    "\n",
    "You should have a folder called `original_images` containing the 2000 images and a file called `miml data.mat` containing the labels.\n",
    "\n",
    "Note, do the steps below __only once__.\n",
    "The dataset download take about five minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-27 22:47:37--  http://www.lamda.nju.edu.cn/files/miml-image-data.rar\n",
      "Resolving www.lamda.nju.edu.cn (www.lamda.nju.edu.cn)... 210.28.132.67\n",
      "Connecting to www.lamda.nju.edu.cn (www.lamda.nju.edu.cn)|210.28.132.67|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25978029 (25M) [application/octet-stream]\n",
      "Saving to: ‘miml-image-data.rar.1’\n",
      "\n",
      "miml-image-data.rar 100%[===================>]  24.77M   109KB/s    in 3m 57s  \n",
      "\n",
      "2020-12-27 22:51:35 (107 KB/s) - ‘miml-image-data.rar.1’ saved [25978029/25978029]\n",
      "\n",
      "\n",
      "UNRAR 5.30 beta 2 freeware      Copyright (c) 1993-2015 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from miml-image-data.rar\n",
      "\n",
      "Extracting  processed.rar                                                  2  OK \n",
      "Extracting  original.rar                                                                1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9100%\n",
      "original.rar         - checksum error\n",
      "Unexpected end of archive\n",
      "Total errors: 2\n",
      "\n",
      "Cannot open original.rar\n",
      "No such file or directory0\n",
      "\n",
      "UNRAR 5.30 beta 2 freeware      Copyright (c) 1993-2015 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from processed.rar\n",
      "\n",
      "Extracting  miml data.mat                                                 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 99  OK \n",
      "All OK\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.lamda.nju.edu.cn/files/miml-image-data.rar\n",
    "!unrar e miml-image-data.rar # gives two rar files\n",
    "!mkdir -p original_images\n",
    "!unrar e original.rar -idq original_images; ls original_images/ | wc -l\n",
    "!unrar e processed.rar  # produces miml data.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "From the PyTorch framework, we import the necessary classes: neural net (nn) models to train the classifier on, optimizers to update the model paramters, image transforms to resize and normalize the images, and metrics generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat  # Load MATLAB file.\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the PyTorch and torchvision version to ensure that they meet our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "torchvision version: 0.8.2\n"
     ]
    }
   ],
   "source": [
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'torchvision version: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Processing\n",
    "Our SceneDataset class inherits from `Dataset` and overrides the following methods (this is typical way to subclass this class) :\n",
    "\n",
    "•\t`__len__` to support returning the size of the Dataset instance.\n",
    "\n",
    "•\t`__getitem__` to support indexing such that the ith sample of an instance of SceneDataset can be retrieved.\n",
    " \n",
    "To the class, we add a `get_labels` method to get the labels associated with an image at index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneDataset(Dataset):\n",
    "    \"\"\" Subclass from Dataset and overide __get_item__ for data-specific indexing\n",
    "     and __len__ to get data-specific length.\n",
    "     We also add a new method, get_labels(index) to avoid going thru an expensive __getitem__\"\"\"\n",
    "     \n",
    "    def __init__(self, df, transforms=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def get_labels(self, idx):\n",
    "        record = self.df.iloc[idx]\n",
    "        return record[1:].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.df.iloc[idx]\n",
    "        image = Image.open(record['filename']).convert(\"RGB\")\n",
    "        label = torch.tensor(record[1:].tolist(), dtype=torch.float32)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We take a stock ResNet50 model with all the pretrained weights initialized and apply a simple extension to the model. First, we create a new head (see method `_create_head`) that consists of three Linear layers.\n",
    "The new head layers can be sumamrized as:\n",
    "```\n",
    "(fc): Sequential(\n",
    "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
    "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "      (3): Dropout(p=0.3, inplace=False)\n",
    "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
    "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (6): ReLU()\n",
    "      (7): Dropout(p=0.3, inplace=False)\n",
    "      (8): Linear(in_features=512, out_features=5, bias=True)\n",
    "    )\n",
    "```\n",
    "We then replace the last fully connected layer with the new head. A conceptual diagram of the final network (`ExtendedResNetModel`) is shown below.\n",
    "\n",
    "![resnet with head](assets/new-rn50-head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedResNetModel(nn.Module):\n",
    "    \"\"\" Extend ResNet with three new fully connected layers and attach them as a head to a ResNet50 trunk\"\"\"\n",
    "\n",
    "    def __init__(self, nb_classes, dropout_prob=0.3, activation_func=nn.ReLU):\n",
    "        super().__init__()\n",
    "        # load the pretrained model as feafures\n",
    "        self.rn50_features = models.resnet50(pretrained=True)\n",
    "        # get the nb of in_features in last Linear unit\n",
    "        nb_in_features_last = self.rn50_features.fc.in_features\n",
    "        for param in self.rn50_features.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        head = self._create_head(nb_in_features_last, nb_classes,\n",
    "                                 dropout_prob, activation_func)\n",
    "        self.rn50_features.fc = head  # attach head\n",
    "        # print(self.rn50_features)\n",
    "\n",
    "    def _create_head(self, nb_features, nb_classes, dropout_prob=0.3, activation_func=nn.ReLU):\n",
    "        features_lst = [nb_features, nb_features//2, nb_features//4]\n",
    "        layers = []\n",
    "        for in_f, out_f in zip(features_lst[:-1], features_lst[1:]):\n",
    "            layers.append(nn.Linear(in_f, out_f))\n",
    "            layers.append(nn.BatchNorm1d(out_f))\n",
    "            layers.append(activation_func())\n",
    "            if dropout_prob != 0:\n",
    "                layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(features_lst[-1], nb_classes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rn50_features(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "A typical training sequence is the following for every batch of data and labels.\n",
    "\n",
    "```\n",
    "    outputs = model(data)  # forward pass\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()  # compute gradient of the loss with respect to model parameters\n",
    "    optimizer.step()  # update the model parameters\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "```                     \n",
    "#### Learning Rate Adjustment\n",
    "We use `lr_scheduler.CosineAnnealingLR` to decay the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer, scheduler, nb_epochs=5):\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        result = []\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == \"train\":  \n",
    "                model.train()  # put model in training mode\n",
    "            else:  \n",
    "                model.eval()  # # put model in validation mode\n",
    "\n",
    "            # Track for each epoch\n",
    "            running_loss = 0.0\n",
    "            running_f1_score = 0.0\n",
    "            running_roc_auc_score = 0.0\n",
    "\n",
    "            for data, targets in data_loader[phase]:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(data)  # forward pass\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    preds = outputs.data > 0.5\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()  # compute gradient of the loss with respect to model parameters\n",
    "                        optimizer.step()  # update the model parameters\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                running_loss += loss.item() * len(data)\n",
    "                running_f1_score += f1_score(targets.to(\"cpu\").to(torch.int).numpy(),\n",
    "                                             preds.to(\"cpu\").to(torch.int).numpy(),\n",
    "                                             average=\"samples\") * len(data)\n",
    "                running_roc_auc_score += roc_auc_score(targets.to(\"cpu\").to(torch.int).numpy(),\n",
    "                                                       preds.to(\"cpu\").to(torch.int).numpy(),\n",
    "                                                       average=\"samples\") * len(data)\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_f1_score = running_f1_score / len(data_loader[phase].dataset)\n",
    "            epoch_roc_auc_score = running_roc_auc_score / \\\n",
    "                len(data_loader[phase].dataset)\n",
    "\n",
    "            result.append(f'Epoch:{epoch} {phase.upper()}: Loss:{epoch_loss:.4f} '\n",
    "                          f'F1-Score: {epoch_f1_score:.4f} AUC: {epoch_roc_auc_score:.4f}')\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Labels\n",
    "The important step to highlight is that the original labels are in the range \\[-1, 1\\] and that are converted to the range \\[0,1\\] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class labels: ['desert', 'mountains', 'sea', 'sunset', 'trees']\n"
     ]
    }
   ],
   "source": [
    "# To download the dataset, see accompanying file, dataset_download_steps.txt.\n",
    "dataset_root = '.'\n",
    "\n",
    "# Read the \"processed\" part for class names and class labels\n",
    "processed_mat = loadmat(os.path.join(dataset_root, 'miml data.mat'))\n",
    "class_labels = []\n",
    "for c in processed_mat['class_name']:  # get the name of each class\n",
    "    class_labels.append(c[0][0])\n",
    "nb_classes = len(class_labels)\n",
    "print('class labels:', class_labels)  # ['desert', 'mountains', 'sea', 'sunset', 'trees']\n",
    "\n",
    "# Read the labels. If multi-class label for ith images equals [1, -1, -1, 1, -1], it means:\n",
    "# i-th image belongs to the 1st & 4th class but does not belong to the 2nd, 3rd &  5th classes\n",
    "labels = copy.deepcopy(processed_mat['targets'].T)\n",
    "labels[labels == -1] = 0  # convert to range [0, 1] from [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Images\n",
    "\n",
    "The code below setups the pandas dataframe, `data_df` with file location and associated multiple labels as below:\n",
    "```\n",
    "                   filename desert mountains sea sunset trees\n",
    " 0  ./original_images/1.jpg      1         0   0      0     0\n",
    " 1  ./original_images/2.jpg      1         0   0      0     0\n",
    " 2  ./original_images/3.jpg      1         0   0      0     0\n",
    " 3  ./original_images/4.jpg      1         1   0      0     0\n",
    " 4  ./original_images/5.jpg      1         0   0      0     0\n",
    "```\n",
    "In addition, the image transforms applied to each image are also defned in `transforms_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe with columns, [filename desert mountains sea sunset tree]\n",
    "data_df = pd.DataFrame(columns=['filename'] + class_labels)\n",
    "filenames = os.listdir(os.path.join(dataset_root, \"original_images/\"))\n",
    "data_df['filename'] = np.array(\n",
    "    sorted(list(map(lambda x: int(Path(x).stem), np.array(filenames)))))\n",
    "data_df['filename'] = data_df['filename'].apply(\n",
    "    lambda x: os.path.join(dataset_root, 'original_images/') + str(x) + '.jpg')\n",
    "data_df[class_labels] = np.array(labels)\n",
    "\n",
    "transforms_list = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                      std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training set and the validation set\n",
    "\n",
    "Highlighting that, when we randomly split the dataset into a training set and a validation set, for repeatable results, it makes sense to initializa the random number generator with `generator=torch.Generator().manual_seed(0)`.\n",
    "\n",
    "Also highlighting that the mumber of negative samples are much larger than the number of positive samples; so we need to compute the ratios (of negative samples to positive samples __per class__) and make them available to the loss function (in the next step).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1400; Val set size: 600\n",
      "Ratio of Negative samples to positive samples per class: tensor([4.0909, 3.5161, 2.3981, 3.2424, 2.5088], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 0.3\n",
    "dataset = SceneDataset(data_df, transforms_list)\n",
    "split_point = int(len(dataset) * split_ratio)\n",
    "trainset, valset = random_split(\n",
    "    dataset, [len(dataset) - split_point, split_point], generator=torch.Generator().manual_seed(0))\n",
    "print(f\"Train set size: {len(trainset)}; Val set size: {len(valset)}\")\n",
    "\n",
    "batch_size = 128\n",
    "dataloader = {\"train\": DataLoader(trainset, shuffle=True, batch_size=batch_size),\n",
    "              \"val\": DataLoader(valset, shuffle=True, batch_size=batch_size)}\n",
    "\n",
    "positive_weights = []\n",
    "for cls in range(nb_classes):\n",
    "    positive_samples = float(sum([dataset.get_labels(idx)[cls] == 1 for idx in trainset.indices]))\n",
    "    negative_samples = float(sum([dataset.get_labels(idx)[cls] == 0 for idx in trainset.indices]))\n",
    "    pos_weight = negative_samples / positive_samples\n",
    "    positive_weights.append(pos_weight)\n",
    "positive_weights = torch.FloatTensor(positive_weights).to('cuda')\n",
    "print('Ratio of Negative samples to positive samples per class:', positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training \n",
    "\n",
    "We assume training will be offloaded to a GPU; so we move the model (and paramters) into the GPU after instantiating it. Note that our loss function is `nn.BCEWithLogitsLoss` (and not `nn.CrossEntropyLoss`) as stated earlier.  \n",
    "\n",
    "For multi-label classification, \"it’s possible to trade off recall and precision by adding weights to positive examples.\"\n",
    "\n",
    "For example, if a dataset contains 100 positive and 300 negative examples of a single class, then `pos_weight` for the class should be equal to 300/100 (=3). The loss would act as if the dataset contains 3 x 100 (=300) positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2846\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2847\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2848\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-85b20e5fe3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                  T_max=5, eta_min=0.005)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgdr_cos_anneal_sched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-68118c324786>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, scheduler, nb_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mrunning_roc_auc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e9c0bfb682b1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt1.2/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2849\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = ExtendedResNetModel(nb_classes=nb_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=positive_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "sgdr_cos_anneal_sched = lr_scheduler.CosineAnnealingLR(optimizer,  # set learning rate schedule\n",
    "                                                 T_max=5, eta_min=0.005)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, sgdr_cos_anneal_sched, nb_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('pyt1.2': conda)",
   "language": "python",
   "name": "python38264bitpyt12condaa6d578b60c2b45e7882cc9b9680228e7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
